---
layout: linear
date: 28 April 2022
title: Pairing Schemes
preamble: |-
    \newcommand{\gen}{\texttt{gen}}
    \newcommand{\spl}{\texttt{spl}}
    \newcommand{\dec}{\texttt{dec}}
    \newcommand{\ECM}{\exists\textrm{CM}}
returnPage: ./
---

Pairings are special binary maps defined on top of groups that allows transferring exponents from one group to the other. They find broad use in cryptography including design of digital signatures.

**Definition.**
Let $$G_1, G_2, G_3 \cong \Int_p$$ be three cyclic groups of order $$p$$. They have the same abstract structure but not necessarily the same description. A *pairing* $$\inner{\cdot}{\cdot}: G_1 \times G_2 \to G_3$$ is a polynomial-time computable map satisfying

- bilinearity: $$\inner{g_1 g_1'}{g_2} = \inner{g_1}{g_2} \inner{g_1'}{g_2}$$ and $$\inner{g_1}{g_2 g_2'} = \inner{g_1}{g_2} \inner{g_1}{g_2'}$$.
- non-degeneracy: $$\inner{g_1}{g_2} \neq 1$$ whenever $$g_1 \neq 1$$ and $$g_2 \neq 1$$.

**Exercise.**
Prove the following properties of a pairing:

- $$\inner{g_1^a}{g_2} = \inner{g_1}{g_2}^a = \inner{g_1}{g_2^a}$$ for all $$a \in \Int$$; that is the exponent could be transferred from one operand to the other. In particular,
    - $$\inner{1}{g_2} = 1 = \inner{g_1}{1}$$;
    - $$\inner{\cdot}{g_2}$$ is a bijection $$G_1 \leftrightarrow G_3$$ if $$g_2 \neq 1$$;
    - $$\inner{g_1}{\cdot}$$ is a bijection $$G_2 \leftrightarrow G_3$$ if $$g_1 \neq 1$$;
- $$\inner{g_1}{g_2} = \inner{g_2}{g_1}$$ if $$G_1 = G_2$$.

In this chapter we only concern about the symmetric case $$G_1 = G_2 =: G$$.

# Boneh-Lynn-Shacham Scheme

Let us take a pairing $$\inner{\cdot}{\cdot}: G \times G \to G'$$ and an arbitrary generator $$g \in G$$. Assume a random oracle $$H: \set{0,1}^* \to G$$. The BLS scheme cannot be simpler: hash-then-sign, and verify via the properties of pairing.

- `gen` generates a random number $$x \in \Int_p$$ and lets $$(sk,pk) := (x, g^x)$$.
- `sgn` returns $$\sigma := H(m)^x$$.
- `vrf` checks if $$\inner{\sigma}{g} = \inner{H(m)}{g^x}$$.

Its soundness is obvious. To prove security, we also need a cryptographic assumption.

> People believe that the Diffie-Hellman problem is hard (i.e. any polynomial time algorithm could succeed with negligible probability only):
> 
> Given $$g^x, g^y$$ where $$x,y \in \Int_p$$ are sampled uniformly, compute $$g^{xy}$$.
> 
> Clearly it can be reduced to discrete logarithm, so its hardness assumption is stronger.

*Security proof.*
Take any $$\ECM^+$$ adversary in the random oracle model. Modify it such that it always requests $$H(m_i)$$ for every message $$m_i$$ which it later requests/forges a signature. Let $$\ell$$ upper bounds the number of requests. We construct a solver for the Diffie-Hellman problem:

- The judge samples $$x,y \in \Int_p$$ uniformly, and transmits $$g^x$$ and $$g^y$$ to us.
- We emulate `gen` by publishing $$pk := g^x$$ to the virtual adversary.
- We guess the index $$j \in [\ell]$$ at which the adversary requests the $$H$$-value for his final forgery. This succeeds with probability $$1/\ell$$, in which case all the following steps can proceed without trouble.
- We emulate request to $$H$$-values by:
    - return the old value if this request has been answered before;
    - return $$g^y$$ if we are at the $$j$$-th request;
    - return $$g^z$$ otherwise, for freshly sampled $$z \in \Int_p$$.
- We emulate `sgn` on message $$m_i$$ by:
    - return the valid signature $$\sigma_i := (g^x)^z$$ where $$z$$ was the fresh randomness that we sampled in response to $$H(m_i)$$.
- On receipt of a valid forgery $$(m, \sigma)$$ from the virtual adversary,
    - note $$\inner{\sigma}{g} = \inner{H(m)}{g^x}$$, and furthermore $$H(m) = g^y$$ provided we guessed the right index $$j$$;
    - hence $$\inner{\sigma}{g} = \inner{g^y}{g^x} = \inner{g^{xy}}{g}$$, so $$\sigma = g^{xy}$$ due to bijectivity;
    - submit $$\sigma$$ to the judge and win the game. âˆŽ

The BLS scheme has the special property of *aggregability*: The messages and signatures coming from $$n$$ players can be bundled into a monolithic message/signature pair for a one-pass verification. For this purpose, we add two specialised routines to the signature scheme:

- $$\texttt{agg}((m_1,\sigma_1), \dots, (m_n,\sigma_n))$$ aggregates $$m := m_1 \Vert \dots \Vert m_n$$ and $$\sigma := \prod_{i=1}^n \sigma_i$$.
- $$\texttt{vrf-agg}(m_1 \Vert \dots \Vert m_n, \sigma)$$ checks if $$\inner{\sigma}{g} = \prod_{i=1}^n \inner{H(m_i)}{g^{x_i}}$$.

The attack protocol needs refinement as well:
- The judge generates independent key pairs $$(sk_i,pk_i) := \gen(1^k)$$ for all $$i \in [n]$$. This mimics the practical situation where $$n$$ signers use BLS scheme. Publish the public keys to the adversary.
- The adversary can dynamically request polynomially many signatures of any signer to any message;
- Finally he comes up with a forgery $$(m,\sigma)$$ that passes the aggregated verification `vrf-agg`.

**Exercise.**
Prove that the BLS scheme is secure in this attack protocol.

# Waters Scheme

Waters scheme, like the BLS scheme, is based on the Diffie-Hellman assumption. But it is constructed in a more sophisticated manner that gives rise to a security proof without random oracle. The key component is a "programmable hash function", which in some way captures the functionality of a random oracle in security proof, by imposing worst-case probability guarantees.

## Programmable hash scheme

**Definition.**
Assume $$\mu: \Nat \to (0,1)$$. A *$$\mu$$-programmable hash scheme* is a tuple $$(\spl, H, \dec)$$:

- The randomised algorithm $$\spl(1^k, g, h)$$ is given two generators $$g,h \in G$$. It samples the descriptor $$\kappa$$ of a concrete hash function and a companion trapdoor $$\tau$$.
- The functor $$H$$ can be instantiated with descriptor $$\kappa$$ into an efficiently-computable function $$H_\kappa: \set{0,1}^k \to G$$.
- The deterministic algorithm $$\dec(\kappa,\tau,m)$$ returns a pair $$(a,b): ~g^a h^b = H_\kappa(m)$$. The decomposition is not unique, of course, but the algorithm deterministically picks one.

Moreover we impose a combinatorial constraint. Fix *arbitrary* generators $$g, h \in G$$ and consider a random experiment where we sample $$(\kappa,\tau) := \spl(1^k, g, h)$$. Note that the randomness originates from the algorithm `spl` itself. For every $$m \in \set{0,1}^k$$ define random variables $$(A_m, B_m) := \dec(\kappa,\tau,m)$$.

For all conditions $$\kappa$$ and any distinct messages $$m, m_1, \dots, m_q \in \set{0,1}^k$$, we require

$$ \Pr \left( \set{B_m=0} \cap \bigcap_{i=1}^{q} \set{B_{m_i} \neq 0} \middle\vert \kappa \right) \geq \mu(q). $$

We stress that the definition is purely combinatorial -- no pairings, no hardness assumptions, nor any security notion.

## A possible construction

We present a simple $$\Theta(\frac{1}{q \sqrt{k}})$$-programmable hash scheme:

- $$\spl(1^k, g, h)$$ samples trapdoor $$\tau := (a_0, \dots, a_k, b_0, \dots, b_k)$$ as described below, then sets descriptor $$\kappa := (u_0, \dots, u_k) := (g^{a_0} h^{b_0}, \dots, g^{a_k} h^{b_k})$$.
    - each $$a_i \in \Int_p$$ is sampled uniformly and independently;
    - each $$b_i \in \Int_p$$ is a sum of $$\Theta(q^2)$$ many i.i.d. variables that are uniformly distributed on $$\set{-1,0,1}$$; in essence we perform an unbiased random walk for $$\Theta(q^2)$$ steps.
- $$H_\kappa(m) := u_0 \cdot \prod_{\text{bit } i \text{ of } m = 1} u_i$$.
- $$\dec(\kappa,\tau,m)$$ naturally decomposes $$(a,b)$$ where $$a := a_0 + \sum_{\text{bit } i \text{ of } m = 1} a_i$$ and $$b := b_0 + \sum_{\text{bit } i \text{ of } m = 1} b_i$$.

Note that the descriptor $$\kappa = (u_0, \dots, u_k)$$ is uniform and independent of variables $$(b_0, \dots, b_k)$$, due to the "erasing effect" of uniform variables $$a_0, \dots, a_k$$. Hence even with the conditioning on $$\kappa$$, the variables $$b_0, \dots, b_k$$ still has the same "random-walk distribution", whose behaviour is described by the lemma below.

**Lemma.**
Let $$X_1, \dots, X_n$$ be i.i.d. random variables following uniform distribution on $$\set{-1,0,1}$$. Denote $$X := \sum_{i=1}^n X_i$$. Then $$\Pr(X=0) = \Theta(1/\sqrt{n})$$.

The proof of the lemma involves heavy computations, which we shall not present. But we invite you to at least write down the exact formula. Also, it is rather easy to prove a lower bound $$\Pr(X=0) \geq \Theta(1/\sqrt{n})$$ by Chebyshev and the observation that the mass drops monotonically from the centre 0.

Assuming the lemma, we are almost done. For any message $$m$$, we have by definition $$B_m =  b_0 + \sum_{\text{bit } i \text{ of } m = 1} b_i$$, which involves at least one and at most $$k+1$$ bulks of random walks. Hence $$\Pr(B_m=0 \mid \kappa) \in [\Theta(\frac{1}{q \cdot \sqrt{k}}), \Theta(\frac{1}{q})]$$. Then we condition on this event and consider any other message $$m_i \neq m$$. Since the two messages have at least one bit difference, this intuitively provides us with enough randomness that smooths the correlation between $$B_m$$ and $$B_{m_i}$$, and one could show that $$\Pr(B_{m_i} = 0 \mid B_m = 0, \kappa) \leq \frac{1}{2q}$$. Applying union bound we have $$\Pr\left( \bigcup_i \set{B_{m_i} = 0} \middle\vert B_m=0, \kappa \right) \leq \frac{1}{2}$$, and the desired lower bound $$\mu(q) = \Theta(\frac{1}{q \sqrt{k}})$$ follows.

## Back to signature scheme

Now we assume a pairing $$\inner{\cdot}{\cdot}: G \times G \to G'$$ and a $$\mu$$-programmable hash scheme $$(\spl,H,\dec)$$ where $$\mu=\mu(q)$$ is non-negligible. Waters signature scheme uses them as blackboxes:

- `gen`:
    - pick a dummy generator $$h \in G$$;
    - sample $$(\kappa,\tau) := \spl(1^k,g,h)$$ and discard $$h, \tau$$ (which are used in security proof only);
    - sample $$sk \in G$$ uniformly;
    - let $$pk := \inner{g}{sk} \Vert \kappa$$.
- `sgn`:
    - sample $$r \in \Int_p$$ uniformly;
    - return $$\sigma := g^r \Vert sk \cdot H_\kappa(m)^r$$.
- `vrf`:
    - unpack $$\sigma =: \rho \Vert \eta$$;
    - check if $$\inner{g}{\eta} = \inner{g}{sk} \cdot \inner{\rho}{H_\kappa(m)}$$.

**Exercises.**

- Justify the soundness.
- Show that if $$(m, \rho \Vert \eta)$$ passes verification, then $$\rho = g^r$$ and $$\eta = sk \cdot H_\kappa(m)^r$$ for some unique $$r \in \Int_p$$. Hence `vrf` is not only sound but in fact complete.

*Security proof.*
We construct a solver for Diffie-Hellman problem based on any $$\ECM^+$$ adversary.

- The judge samples $$x,y \in \Int_p$$ uniformly, and transmits $$g^x$$ and $$g^y$$ to us.
- We emulate `gen` by
    - let $$h := g^x$$;
    - sample $$(\kappa,\tau) := \spl(1^k, g, h)$$;
    - imagine $$sk := g^{xy}$$;
    - publish $$pk := \inner{g^x}{g^y} \Vert \kappa$$ to the virtual adversary.
- Observe that $$sk$$ (equivalently $$\inner{g^x}{g^y}$$) is independent of $$h$$ (thus also $$\tau$$). Therefore, with regard to the trapdoor $$\tau$$, neither secret/public key nor any signature reveals more information other than $$\kappa$$ alone. Hence the adversary indeed lives in a probability space where $$\kappa$$ is the only condition, just as in the specification of programmable hash function. (Alternative explanation: it is by no means that the signature scheme itself helps one break the probability lower bound $$\mu$$. Otherwise one can simulate the signature scheme *himself* for designing messages that break the lower bound.)
- The secret key is imaginary and unknown, so we would come to trouble in `sgn`. The trick is to utilize the trapdoor to decompose $$H(m_i) = g^a h^b = g^{a+bx}$$. By programmability we can assume $$b \neq 0$$ (with descent probability). We can thus artificially set $$r := r' - y/b$$ (with uniform $$r' \in \Int_p$$ to ensure correct distribution), so that $$H(m_i)^r = g^{ar+brx} = g^{-xy} \cdot g^{ar'} (g^x)^{br'} (g^y)^{-a/b}$$ contains a factor that cancels $$sk$$ and save us. Of course we don't know $$y$$ (and hence $$r$$) explicitly, but by sampling $$r'$$ we conceptually determine a uniform $$r$$ as required. Formally, we emulate `sgn` by:
    - let $$(a,b) := \dec(\kappa,\tau, m_i)$$;
    - sample $$r' \in \Int_p$$ uniformly (which implicitly determines $$r = r'-y/b$$);
    - return $$\sigma := g^{r'} (g^y)^{-1/b} ~\Vert~ g^{ar'} (g^x)^{br'} (g^y)^{-a/b}$$.
- Finally, upon receiving a valid forgery $$(m, \rho \Vert \eta)$$,
    - imagine $$\rho = g^r$$ and $$\eta = sk \cdot H_\kappa(m)^r$$;
    - let $$(a,b) := \dec(\kappa,\tau, m_i)$$, where we can assume $$b=0$$ with descent probability;
    - hence $$\eta = sk \cdot g^{ar} = sk \cdot \sigma^a$$;
    - so we submit $$sk = \eta \cdot \sigma^{-a}$$ to the judge and win the game. âˆŽ